import os, time, json, re
import gspread
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# [ì„¤ì •] ì˜¤í¼ì„¼íŠ¸ ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ì „ìš© ì •ë³´
CONFIG = {
    "name": "ì˜¤í¼ì„¼íŠ¸_ì‹ ê·œë¦¬ìŠ¤íŠ¸",
    "url": "https://offercent.co.kr/list?jobCategories=0040002%2C0170004&sort=recent",
    "gid": "639559541"  # ê¸°ì¡´ ì‹œíŠ¸ GID ìœ ì§€ (í•„ìš”ì‹œ ë³€ê²½)
}

# [ê³µí†µ] ì‹œíŠ¸ ì—°ê²°
def get_worksheet():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds_dict = json.loads(os.environ['GOOGLE_CREDENTIALS'])
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    spreadsheet = client.open_by_url("https://docs.google.com/spreadsheets/d/1nKPVCZ6zAOfpqCjV6WfjkzCI55FA9r2yvi9XL3iIneo/edit")
    
    sheet = next((s for s in spreadsheet.worksheets() if str(s.id) == CONFIG["gid"]), None)
    if not sheet: raise Exception(f"{CONFIG['gid']} ì‹œíŠ¸ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return sheet

# [ê³µí†µ] ë¸Œë¼ìš°ì € ì‹¤í–‰ ì„¤ì •
def get_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    driver = webdriver.Chrome(options=options)
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
    })
    return driver

# [ì „ìš©] ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ (ì œê³µí•´ì£¼ì‹  HTML êµ¬ì¡° ë°˜ì˜)
def scrape_projects():
    driver = get_driver()
    driver.set_window_size(1920, 1080) # ì‹¤í–‰ ì°½ í¬ê¸° ëª…ì‹œ
    new_data = []
    today = datetime.now().strftime("%Y-%m-%d")
    
    try:
        print(f"ğŸ”— ì ‘ì† ì¤‘: {CONFIG['url']}")
        driver.get(CONFIG["url"])
        
        # 1. í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ê°•í™”
        time.sleep(10) # ì¶©ë¶„í•œ ì´ˆê¸° ë¡œë”© ì‹œê°„ ë¶€ì—¬
        
        # 2. ê³µê³  ì¹´ë“œê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ ì²´í¬
        cards = driver.find_elements(By.CSS_SELECTOR, "a[href*='/jd/']")
        print(f"ğŸ” ë°œê²¬ëœ ê³µê³  ì¹´ë“œ ê°œìˆ˜: {len(cards)}ê°œ")

        if len(cards) == 0:
            # ì¹´ë“œê°€ ì—†ë‹¤ë©´ í˜ì´ì§€ ì†ŒìŠ¤ ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            print("â— ê³µê³  ì¹´ë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì„ íƒìë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return []

        for card in cards:
            try:
                href = card.get_attribute("href")
                title = card.text.strip()
                
                # ë¶€ëª¨ ìš”ì†Œë¥¼ ëª» ì°¾ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”
                try:
                    # ì œê³µí•˜ì‹  HTML êµ¬ì¡°ìƒ aíƒœê·¸ ìƒìœ„ì— ì •ë³´ê°€ ìˆìœ¼ë¯€ë¡œ íƒìƒ‰ ì‹œë„
                    # ë§Œì•½ ì•„ë˜ êµ¬ë¬¸ì—ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´ í…ìŠ¤íŠ¸ë¥¼ ëª» ê°€ì ¸ì˜µë‹ˆë‹¤.
                    parent = card.find_element(By.XPATH, "./ancestor::div[contains(@class, 'x1n2onr6')][1]")
                    company = parent.find_element(By.CSS_SELECTOR, 'span[data-variant="body-02"]').text.strip()
                    info = parent.find_element(By.CSS_SELECTOR, 'span[data-variant="body-03"]').text.strip()
                    
                    print(f"âœ… ìˆ˜ì§‘ ì„±ê³µ: {company} - {title}")
                    
                    # (ì´í•˜ ê¸°ì¡´ ë¶„ë¦¬ ë¡œì§ ë™ì¼...)
                    location = info.split('Â·')[0].strip() if 'Â·' in info else info
                    experience = info.split('Â·')[1].strip() if 'Â·' in info else ""
                    
                    new_data.append({
                        'company': company, 'title': title, 'location': location,
                        'experience': experience, 'url': href, 'scraped_at': today
                    })
                except Exception as inner_e:
                    print(f"âš ï¸ ê°œë³„ ì¹´ë“œ ë¶„ì„ ì‹¤íŒ¨ ({title}): {inner_e}")
                    continue
            except:
                continue
    finally: 
        driver.quit()
    return new_data
    
# [ê³µí†µ] ì‹œíŠ¸ ë°ì´í„° ì—…ë°ì´íŠ¸
def update_sheet(ws, data):
    if not data: 
        print(f"[{CONFIG['name']}] ìƒˆë¡œ ìˆ˜ì§‘ëœ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    all_v = ws.get_all_values()
    # í—¤ë”ì— locationê³¼ experienceê°€ ì¶”ê°€ë¨
    headers = all_v[0] if all_v else ['company', 'title', 'location', 'experience', 'url', 'scraped_at', 'status']
    
    col_map = {name: i for i, name in enumerate(headers)}
    existing_urls = {row[col_map['url']] for row in all_v[1:] if len(row) > col_map['url']}
    
    rows_to_append = []
    for item in data:
        if item['url'] in existing_urls: continue
        
        row = [''] * len(headers)
        for k, v in item.items():
            if k in col_map: row[col_map[k]] = v
        
        if 'status' in col_map: row[col_map['status']] = 'new'
        rows_to_append.append(row)
    
    if rows_to_append:
        ws.append_rows(rows_to_append)
        print(f"ğŸ’¾ {CONFIG['name']} ì‹ ê·œ ê³µê³  {len(rows_to_append)}ê±´ ì €ì¥ ì™„ë£Œ")

if __name__ == "__main__":
    try:
        ws = get_worksheet()
        data = scrape_projects()
        update_sheet(ws, data)
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
