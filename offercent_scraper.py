import os, time, json, re
import gspread
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# [ì„¤ì •] ì´ íŒŒì¼ ì „ìš© ì •ë³´
CONFIG = {
    "name": "ì˜¤í¼ì„¼íŠ¸",
    "url": "https://offercent.co.kr/company-list?jobCategories=0040002%2C0170004",
    "gid": "639559541" # ì˜¤í¼ì„¼íŠ¸ íƒ­
}

# [ê³µí†µ] ì‹œíŠ¸ ì—°ê²° (GIDë¡œ ì°¾ê¸°)
def get_worksheet():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds_dict = json.loads(os.environ['GOOGLE_CREDENTIALS'])
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    spreadsheet = client.open_by_url("https://docs.google.com/spreadsheets/d/1nKPVCZ6zAOfpqCjV6WfjkzCI55FA9r2yvi9XL3iIneo/edit")
    # ìˆœì„œê°€ ë°”ë€Œì–´ë„ IDë¡œ íƒ­ì„ ì°¾ìŒ
    sheet = next((s for s in spreadsheet.worksheets() if str(s.id) == CONFIG["gid"]), None)
    if not sheet: raise Exception(f"{CONFIG['gid']} ì‹œíŠ¸ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return sheet

# [ê³µí†µ] ë¸Œë¼ìš°ì € ì‹¤í–‰
def get_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-blink-features=AutomationControlled")
    driver = webdriver.Chrome(options=options)
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {"source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"})
    return driver

# [ì „ìš©] ë°ì´í„° ìˆ˜ì§‘ - ìµœì¢… ë³´ì •íŒ
def scrape_projects():
    driver = get_driver()
    new_data = []
    today = datetime.now().strftime("%Y-%m-%d")
    urls_check = set()
    
    try:
        driver.get(CONFIG["url"])
        # íƒ€ìž„ì•„ì›ƒ ì—ëŸ¬ ë°©ì§€: ìš”ì†Œ í•˜ë‚˜ë§Œ ë‚˜íƒ€ë‚˜ë„ ì¦‰ì‹œ ì‹¤í–‰
        wait = WebDriverWait(driver, 15)
        try:
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "a")))
        except:
            print("âš ï¸ ë¡œë”© ì§€ì—° ë°œìƒ - ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

        # ëˆ„ë½ ë°©ì§€ë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ìŠ¤í¬ë¡¤ (10íšŒ)
        for i in range(10):
            # í˜„ìž¬ íŽ˜ì´ì§€ì— ì¡´ìž¬í•˜ëŠ” ëª¨ë“  ì¹´ë“œ(a íƒœê·¸) íšë“
            cards = driver.find_elements(By.TAG_NAME, "a")
            
            for card in cards:
                href = card.get_attribute("href")
                # ìƒì„¸ ê³µê³  íŽ˜ì´ì§€(/job/) ë§í¬ì¸ì§€ í™•ì¸
                if not href or "/job/" not in href: continue
                
                try:
                    # ì¹´ë“œ ë‚´ë¶€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸(span) ì¶”ì¶œ
                    all_spans = card.find_elements(By.CSS_SELECTOR, "span.greet-typography")
                    
                    company = ""
                    title_list = []
                    
                    for s in all_spans:
                        cls = s.get_attribute("class") or ""
                        txt = s.text.strip()
                        if not txt or "ì±„ìš© ì¤‘ì¸ ê³µê³ " in txt: continue
                        
                        # ì œëª© í´ëž˜ìŠ¤(xlyipyv)ê°€ ìžˆìœ¼ë©´ ì œëª© ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                        if "xlyipyv" in cls:
                            title_list.append(txt)
                        # ì œëª©ì´ ì•„ë‹ˆê³  ì•„ì§ íšŒì‚¬ëª…ì´ ë¹„ì–´ìžˆë‹¤ë©´ íšŒì‚¬ëª…ìœ¼ë¡œ ì €ìž¥
                        elif not company:
                            company = txt

                    # í•œ ì¹´ë“œ ë‚´ì˜ ì—¬ëŸ¬ ì œëª© ì²˜ë¦¬
                    for title in title_list:
                        unique_id = f"{href}_{title}"
                        if unique_id not in urls_check:
                            new_data.append({
                                'company': company,
                                'title': title,
                                'url': href,
                                'scraped_at': today
                            })
                            urls_check.add(unique_id)
                except:
                    continue
            
            # ìŠ¤í¬ë¡¤ í›„ ìƒˆë¡œìš´ ì½˜í…ì¸ ê°€ ë¡œë“œë  ì‹œê°„ì„ ì¤Œ (ë¹„ì— ìŠ¤ë§ˆì¼ ëˆ„ë½ ë°©ì§€)
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3) 

    finally: 
        driver.quit()
    
    print(f"âœ… ì´ {len(new_data)}ê±´ì˜ ê³µê³ ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    return new_data
    
# [ê³µí†µ] ìŠ¤ë§ˆíŠ¸ ì €ìž¥ (í—¤ë” ì´ë¦„ ê¸°ì¤€)
def update_sheet(ws, data):
    if not data: return print(f"[{CONFIG['name']}] ìƒˆ ê³µê³  ì—†ìŒ")
    all_v = ws.get_all_values()
    headers = all_v[0] if all_v else ['title', 'url', 'scraped_at', 'status', 'location']
    col_map = {name: i for i, name in enumerate(headers)}
    existing_urls = {row[col_map['url']] for row in all_v[1:] if len(row) > col_map['url']}
    
    rows = []
    for item in data:
        if item['url'] in existing_urls: continue
        row = [''] * len(headers)
        for k, v in item.items():
            if k in col_map: row[col_map[k]] = v
        if 'status' in col_map: row[col_map['status']] = 'archived'
        rows.append(row)
    
    if rows: ws.append_rows(rows); print(f"ðŸ’¾ {CONFIG['name']} {len(rows)}ê±´ ì €ìž¥")

if __name__ == "__main__":
    ws = get_worksheet(); data = scrape_projects(); update_sheet(ws, data)
