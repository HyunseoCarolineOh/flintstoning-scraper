def get_projects():
    driver = get_driver()
    new_data = []
    today = datetime.now().strftime("%Y-%m-%d")

    # [ì¤‘ë³µ ë°©ì§€ìš©] ì´ë¯¸ ìˆ˜ì§‘í•œ URLì„ ë¹ ë¥´ê²Œ ì°¾ê¸° ìœ„í•´ set ì‚¬ìš©
    collected_urls = set()

    try:
        print("ğŸŒ ì˜¤í¼ì„¼íŠ¸ ì ‘ì† ì¤‘...")
        driver.get(SCRAPE_URL)
        
        wait = WebDriverWait(driver, 20)
        wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(3) 

        # ---------------------------------------------------------
        # ë‚´ë¶€ í•¨ìˆ˜: í˜„ì¬ í™”ë©´ì— ë³´ì´ëŠ” ê³µê³ ë“¤ì„ ê¸ì–´ëª¨ìœ¼ëŠ” ë¡œì§
        # ---------------------------------------------------------
        def scrape_current_view():
            elements = driver.find_elements(By.TAG_NAME, "a")
            count = 0
            
            BAD_KEYWORDS = ["ì±„ìš© ì¤‘ì¸ ê³µê³ ", "ì±„ìš©ë§ˆê°", "ë§ˆê°ì„ë°•", "ìƒì‹œì±„ìš©", "NEW", "D-"]

            for elem in elements:
                try:
                    full_url = elem.get_attribute("href")
                    # URLì´ ì—†ê±°ë‚˜, ìê¸° ìì‹ (ë©”ì¸)ì´ê±°ë‚˜, ì´ë¯¸ ìˆ˜ì§‘í•œ URLì´ë©´ íŒ¨ìŠ¤
                    if not full_url or full_url == SCRAPE_URL or full_url in collected_urls: 
                        continue
                    
                    raw_text = elem.text.strip()
                    if not raw_text: continue

                    lines = raw_text.split('\n')
                    cleaned_lines = []
                    
                    for line in lines:
                        text = line.strip()
                        if not text: continue
                        
                        is_bad = False
                        for bad in BAD_KEYWORDS:
                            if bad in text:
                                is_bad = True
                                break
                        if not is_bad:
                            cleaned_lines.append(text)

                    if len(cleaned_lines) < 2: continue

                    company = cleaned_lines[0]
                    title = cleaned_lines[1]

                    if len(title) <= 3 and len(cleaned_lines) > 2:
                        title = cleaned_lines[2]

                    if len(title) > 1 and len(company) > 1:
                        new_data.append({
                            'title': title,
                            'company': company,
                            'url': full_url,
                            'scraped_at': today
                        })
                        collected_urls.add(full_url) # ìˆ˜ì§‘ ëª©ë¡ì— ë“±ë¡
                        count += 1
                except:
                    continue
            return count
        # ---------------------------------------------------------

        print("â¬‡ï¸ ìŠ¤í¬ë¡¤ê³¼ ë™ì‹œì— ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0
        
        while True:
            # 1. [ì¤‘ìš”] ìŠ¤í¬ë¡¤ ë‚´ë¦¬ê¸° ì „ì— ì¼ë‹¨ í˜„ì¬ ë³´ì´ëŠ” ê²ƒë“¤ ìˆ˜ì§‘! (ë§¨ ìœ„ ê³µê³  í™•ë³´)
            found = scrape_current_view()
            # print(f"   (ìŠ¤í¬ë¡¤ ì „/í›„ ìˆ˜ì§‘ëœ ê°œìˆ˜: {found}ê°œ)")
            
            # 2. ìŠ¤í¬ë¡¤ ë‹¤ìš´
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3) # ë¡œë”© ëŒ€ê¸°
            
            # 3. ë†’ì´ ë¹„êµ (ë” ë‚´ë ¤ê°”ë‚˜?)
            new_height = driver.execute_script("return document.body.scrollHeight")
            scroll_count += 1
            print(f"   ...ìŠ¤í¬ë¡¤ {scroll_count}íšŒ ì§„í–‰ (ëˆ„ì  ìˆ˜ì§‘: {len(new_data)}ê°œ)")

            if new_height == last_height:
                # í˜¹ì‹œ ë§ˆì§€ë§‰ ë¡œë”© í›„ ë†“ì¹œ ê²Œ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ í•œ ë²ˆ ë” ìˆ˜ì§‘
                scrape_current_view()
                print("ğŸ í˜ì´ì§€ ë ë„ë‹¬")
                break
                
            last_height = new_height
                
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì—ëŸ¬: {e}")
    finally:
        driver.quit()
            
    print(f"ğŸ¯ ìµœì¢… ìˆ˜ì§‘ëœ ê³µê³ : {len(new_data)}ê°œ")
    
    if len(new_data) > 0:
        print("ğŸ“Š [ìƒ˜í”Œ ë°ì´í„°]")
        for i in range(min(3, len(new_data))):
             print(f"   ì œëª©: {new_data[i]['title']} / íšŒì‚¬: {new_data[i]['company']}")

    return new_data
