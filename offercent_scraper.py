def get_projects():
    driver = get_driver()
    new_data = []
    today = datetime.now().strftime("%Y-%m-%d")

    try:
        print("ğŸŒ ì˜¤í¼ì„¼íŠ¸ ì ‘ì† ì¤‘...")
        driver.get(SCRAPE_URL)
        
        wait = WebDriverWait(driver, 20)
        wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(5) 

        for i in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        elements = driver.find_elements(By.TAG_NAME, "a")
        print(f"ğŸ” ë°œê²¬ëœ ì „ì²´ ë§í¬ ìˆ˜: {len(elements)}ê°œ")

        # ë¬´ì‹œí•  í‚¤ì›Œë“œ (ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ë˜ë©´ í•´ë‹¹ ì¤„ë§Œ ì œì™¸)
        # "ì±„ìš© ì¤‘ì¸ ê³µê³ "ëŠ” ë°°ì§€ í…ìŠ¤íŠ¸ì´ë¯€ë¡œ ì œì™¸
        IGNORE_EXACT_MATCH = ["ì±„ìš© ì¤‘ì¸ ê³µê³ ", "ì±„ìš©ë§ˆê°", "ë§ˆê°ì„ë°•", "ìƒì‹œì±„ìš©", "NEW"]

        for elem in elements:
            try:
                full_url = elem.get_attribute("href")
                if not full_url or full_url == SCRAPE_URL: continue
                
                raw_text = elem.text.strip()
                if not raw_text: continue

                lines = raw_text.split('\n')
                cleaned_lines = []
                
                # í•œ ì¤„ì”© ê²€ì‚¬
                for line in lines:
                    text = line.strip()
                    if not text: continue
                    
                    # ë°°ì§€/ìƒíƒœ í…ìŠ¤íŠ¸ ì œê±° ë¡œì§
                    should_skip = False
                    for kw in IGNORE_EXACT_MATCH:
                        if kw in text:  # í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê±´ë„ˆëœ€
                            should_skip = True
                            break
                    # ë‚ ì§œ í˜•ì‹(D-ìˆ«ì) ì œê±°
                    if text.startswith("D-") and len(text) < 6:
                        should_skip = True

                    if not should_skip:
                        cleaned_lines.append(text)

                # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´(íšŒì‚¬ëª…ë§Œ ìˆê±°ë‚˜ ë“±) ìŠ¤í‚µ
                if len(cleaned_lines) < 2:
                    continue

                # ìˆœì„œ ì¶”ì •: ë³´í†µ [íšŒì‚¬ëª…, ì œëª©] ìˆœì„œ
                company = cleaned_lines[0]
                title = cleaned_lines[1]

                # ë§Œì•½ ì²«ì§¸ ì¤„ì´ ì¹´í…Œê³ ë¦¬(ì˜ˆ: "ë§ˆì¼€íŒ…") ê°™ê³  ì…‹ì§¸ ì¤„ì´ ìˆë‹¤ë©´ ì¡°ì •
                # (ì˜¤í¼ì„¼íŠ¸ëŠ” íšŒì‚¬ëª…ì´ ë¨¼ì € ë‚˜ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ ê¸°ë³¸ì€ 0:íšŒì‚¬, 1:ì œëª©)
                
                # ì œëª© ìœ íš¨ì„± ì²´í¬ (ë„ˆë¬´ ì§§ìœ¼ë©´ ë‹¤ìŒ ì¤„ í™•ì¸)
                if len(title) < 2 and len(cleaned_lines) > 2:
                    title = cleaned_lines[2]

                # ìµœì¢… ì €ì¥ ì¡°ê±´
                if len(title) > 1 and len(company) > 0:
                    # ì¤‘ë³µ ì²´í¬ (í˜„ì¬ ìˆ˜ì§‘ ëª©ë¡ ë‚´ì—ì„œ)
                    if not any(d['url'] == full_url for d in new_data):
                        # [ë””ë²„ê¹…] ë¬´ì—‡ì„ ìˆ˜ì§‘í–ˆëŠ”ì§€ ì¶œë ¥
                        print(f"  âœ… ìˆ˜ì§‘í•¨: [{company}] {title}")
                        new_data.append({
                            'title': title,
                            'company': company,
                            'url': full_url,
                            'scraped_at': today
                        })
            except Exception:
                continue
                
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì—ëŸ¬: {e}")
    finally:
        driver.quit()
            
    print(f"ğŸ¯ ìµœì¢… ìˆ˜ì§‘ëœ ê³µê³ : {len(new_data)}ê°œ")
    return new_data
