def get_projects():
    driver = get_driver()
    new_data = []
    today = datetime.now().strftime("%Y-%m-%d")

    try:
        print("ğŸŒ ì˜¤í¼ì„¼íŠ¸ ì ‘ì† ì¤‘...")
        driver.get(SCRAPE_URL)
        
        wait = WebDriverWait(driver, 20)
        wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(5) 

        # ìŠ¤í¬ë¡¤ ë‹¤ìš´
        for i in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        elements = driver.find_elements(By.TAG_NAME, "a")
        print(f"ğŸ” íƒìƒ‰ëœ ë§í¬ ìˆ˜: {len(elements)}ê°œ")

        # [ìˆ˜ì •] ë¬´ì‹œí•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì—¬ê¸°ì— í¬í•¨ëœ ì¤„ì€ ë°ì´í„°ë¡œ ì“°ì§€ ì•ŠìŒ)
        IGNORE_KEYWORDS = ["ì±„ìš© ì¤‘ì¸ ê³µê³ ", "ì±„ìš©ë§ˆê°", "ë§ˆê°ì„ë°•", "ìƒì‹œì±„ìš©", "D-", "NEW"]

        for elem in elements:
            try:
                full_url = elem.get_attribute("href")
                if not full_url or full_url == SCRAPE_URL: continue
                
                raw_text = elem.text.strip()
                if not raw_text: continue

                lines = raw_text.split('\n')
                
                # [í•µì‹¬ ìˆ˜ì •] ì˜ë¯¸ ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¸°ê¸° (í•„í„°ë§)
                cleaned_lines = []
                for line in lines:
                    text = line.strip()
                    if not text: continue
                    
                    # "ì±„ìš© ì¤‘ì¸ ê³µê³ " ê°™ì€ ìƒíƒœ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ê±´ë„ˆëœ€
                    is_ignored = False
                    for keyword in IGNORE_KEYWORDS:
                        if keyword in text:
                            is_ignored = True
                            break
                    
                    if not is_ignored:
                        cleaned_lines.append(text)
                
                # í•„í„°ë§ í›„ì—ë„ ë°ì´í„°ê°€ 2ì¤„ ì´ìƒì´ì–´ì•¼ í•¨ (íšŒì‚¬ëª… + ì œëª©)
                if len(cleaned_lines) < 2: continue

                # ì´ì œ 0ë²ˆì§¸ê°€ íšŒì‚¬ëª…, 1ë²ˆì§¸ê°€ ì§„ì§œ ì œëª©ì¼ í™•ë¥ ì´ ë§¤ìš° ë†’ìŒ
                company = cleaned_lines[0]
                title = cleaned_lines[1]

                # ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´(3ê¸€ì ì´í•˜) ê·¸ ë‹¤ìŒ ì¤„ì„ ì œëª©ìœ¼ë¡œ ì‹œë„
                if len(title) <= 3 and len(cleaned_lines) >= 3:
                    title = cleaned_lines[2]

                if len(title) > 2 and len(company) > 1:
                    if not any(d['url'] == full_url for d in new_data):
                        new_data.append({
                            'title': title,
                            'company': company,
                            'url': full_url,
                            'scraped_at': today
                        })
            except Exception:
                continue
                
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì—ëŸ¬: {e}")
    finally:
        driver.quit()
            
    print(f"ğŸ¯ ìˆ˜ì§‘ëœ ê³µê³ : {len(new_data)}ê°œ")
    
    # [ë””ë²„ê¹…] ì‹¤ì œ ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ í™•ì¸
    if len(new_data) > 0:
        print("ğŸ“Š [ìƒ˜í”Œ ë°ì´í„° í™•ì¸]")
        for i in range(min(3, len(new_data))):
            print(f"   ì œëª©: {new_data[i]['title']} | íšŒì‚¬: {new_data[i]['company']}")

    return new_data
