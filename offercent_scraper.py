import os, time, json, re
import gspread
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ==========================================
# [ì „ìš©] ì„¤ì • ì •ë³´
# ==========================================
CONFIG = {
    "name": "ì˜¤í¼ì„¼íŠ¸_í†µí•©_í¬ë¡¤ëŸ¬",
    "url": "https://offercent.co.kr/list?jobCategories=0040002%2C0170004&sort=recent",
    "gid": "639559541"
}

# ==========================================
# [ê³µí†µ] êµ¬ê¸€ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—°ê²° ë¡œì§
# ==========================================
def get_worksheet():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds_dict = json.loads(os.environ['GOOGLE_CREDENTIALS'])
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    spreadsheet = client.open_by_url("https://docs.google.com/spreadsheets/d/1nKPVCZ6zAOfpqCjV6WfjkzCI55FA9r2yvi9XL3iIneo/edit")
    sheet = next((s for s in spreadsheet.worksheets() if str(s.id) == CONFIG["gid"]), None)
    if not sheet: raise Exception(f"{CONFIG['gid']} ì‹œíŠ¸ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return sheet

# ==========================================
# [ê³µí†µ] ì…€ë ˆë‹ˆì›€ ë¸Œë¼ìš°ì € ì„¤ì • ë¡œì§
# ==========================================
def get_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("window-size=1920,1080")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    driver = webdriver.Chrome(options=options)
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
    })
    return driver

# ==========================================
# [ì „ìš©] ì˜¤í¼ì„¼íŠ¸ ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ (ìŠ¤í¬ë¡¤ ê°•í™” ë²„ì „)
# ==========================================
def scrape_projects():
    driver = get_driver()
    new_data = []
    today = datetime.now().strftime("%Y-%m-%d")
    urls_check = set()
    
    try:
        print(f"ğŸ”— ì ‘ì† ì¤‘: {CONFIG['url']}")
        driver.get(CONFIG["url"])
        wait = WebDriverWait(driver, 20)
        
        # [ì „ìš© ì„ íƒì] ì œëª© í´ë˜ìŠ¤ xqzk367ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "a.xqzk367")))
        
        # ------------------------------------------------------
        # ë¬´í•œ ìŠ¤í¬ë¡¤ ë¡œì§: ë” ì´ìƒ ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ì„ ë•Œê¹Œì§€ ë‚´ë¦¼
        # ------------------------------------------------------
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0
        max_scrolls = 15  # ìˆ˜ì§‘ëŸ‰ì— ë”°ë¼ ì´ ìˆ«ìë¥¼ ëŠ˜ë¦¬ì„¸ìš”.
        
        print("ğŸ“¥ ëª¨ë“  ê³µê³ ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ ìŠ¤í¬ë¡¤ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        while scroll_count < max_scrolls:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)  # ë¡œë”© ëŒ€ê¸° (ì‚¬ì´íŠ¸ ì†ë„ì— ë”°ë¼ 2~4ì´ˆ ì¡°ì ˆ)
            
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                print("ğŸ ë” ì´ìƒ ë¶ˆëŸ¬ì˜¬ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            last_height = new_height
            scroll_count += 1
            print(f"ğŸ”„ ìŠ¤í¬ë¡¤ ì¤‘... ({scroll_count}/{max_scrolls})")

        # ìŠ¤í¬ë¡¤ ì™„ë£Œ í›„ ì „ì²´ ì¹´ë“œ ë¦¬ìŠ¤íŠ¸ í™•ë³´
        cards = driver.find_elements(By.CSS_SELECTOR, "a.xqzk367[href*='/jd/']")
        print(f"ğŸ” ì´ ë°œê²¬ëœ ê³µê³  ì¹´ë“œ ê°œìˆ˜: {len(cards)}ê°œ")

        for card in cards:
            try:
                title = card.text.strip()
                full_href = card.get_attribute("href")
                clean_url = full_href.split('?')[0] # URL íŒŒë¼ë¯¸í„° ì •ì œ
                
                # [ìœ ì—°í•œ íƒìƒ‰] aíƒœê·¸ì˜ ë¶€ëª¨ ìš”ì†Œë¥¼ íƒ€ê³  ì˜¬ë¼ê°€ë©° ì •ë³´ íƒìƒ‰
                container = card.find_element(By.XPATH, "..") 
                
                company_name = "íšŒì‚¬ëª… ë¯¸ìƒ"
                location = ""
                experience = ""

                # ìƒìœ„ë¡œ 5ë‹¨ê³„ê¹Œì§€ë§Œ ì˜¬ë¼ê°€ë©° íšŒì‚¬ëª…(body-02)ê³¼ ì •ë³´(body-03)ê°€ ìˆëŠ”ì§€ í™•ì¸
                for _ in range(5):
                    try:
                        # 1. íšŒì‚¬ëª… ì°¾ê¸° (body-02)
                        company_el = container.find_element(By.CSS_SELECTOR, 'span[data-variant="body-02"]')
                        company_name = company_el.text.strip()
                        
                        # 2. ì§€ì—­/ê²½ë ¥ ì°¾ê¸° (body-03)
                        info_el = container.find_element(By.CSS_SELECTOR, 'span[data-variant="body-03"]')
                        info_text = info_el.text.strip()
                        
                        if "Â·" in info_text:
                            parts = info_text.split("Â·")
                            location, experience = parts[0].strip(), parts[1].strip()
                        else:
                            location = info_text
                        
                        if company_name != "íšŒì‚¬ëª… ë¯¸ìƒ" and location:
                            break
                    except:
                        container = container.find_element(By.XPATH, "..")

                # ì¤‘ë³µ ë°ì´í„° ìˆ˜ì§‘ ë°©ì§€
                data_id = f"{clean_url}_{title}"
                if data_id not in urls_check:
                    new_data.append({
                        'company': company_name,
                        'title': title,
                        'location': location,
                        'experience': experience,
                        'url': clean_url,
                        'scraped_at': today
                    })
                    urls_check.add(data_id)
                    # ìƒì„¸ ë¡œê·¸ëŠ” ë„ˆë¬´ ë§ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ìƒëµí•˜ê±°ë‚˜ í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
                    # print(f"âœ… ì¶”ì¶œ: {company_name} | {title}")

            except Exception:
                continue

    finally: 
        driver.quit()
    
    print(f"ğŸ“¦ ìµœì¢… ìˆ˜ì§‘ ì™„ë£Œëœ ê³µê³ : {len(new_data)}ê±´")
    return new_data
# ==========================================
# [ê³µí†µ] ì‹œíŠ¸ ë°ì´í„° ì—…ë°ì´íŠ¸ ë¡œì§
# ==========================================
def update_sheet(ws, data):
    if not data: 
        print(f"[{CONFIG['name']}] ìƒˆë¡œ ìˆ˜ì§‘ëœ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    all_v = ws.get_all_values()
    headers = all_v[0] if all_v else ['company', 'title', 'location', 'experience', 'url', 'scraped_at', 'status']
    
    col_map = {name: i for i, name in enumerate(headers)}
    # ê¸°ì¡´ ë°ì´í„° ì¤‘ë³µ ë¹„êµ (URL íŒŒë¼ë¯¸í„° ì œì™¸)
    existing_urls = {row[col_map['url']].split('?')[0] for row in all_v[1:] if len(row) > col_map['url']}
    
    rows_to_append = []
    for item in data:
        if item['url'] in existing_urls: continue
        row = [''] * len(headers)
        for k, v in item.items():
            if k in col_map: row[col_map[k]] = v
        if 'status' in col_map: row[col_map['status']] = 'new'
        rows_to_append.append(row)
    
    if rows_to_append:
        ws.append_rows(rows_to_append)
        print(f"ğŸ’¾ {CONFIG['name']} ì‹ ê·œ ê³µê³  {len(rows_to_append)}ê±´ ì €ì¥ ì™„ë£Œ")

# ==========================================
# [ê³µí†µ] ì‹¤í–‰ ë©”ì¸ ë£¨í‹´
# ==========================================
if __name__ == "__main__":
    try:
        ws = get_worksheet()
        data = scrape_projects()
        update_sheet(ws, data)
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
